{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from os import listdir\n",
    "import cv2 \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array, ImageDataGenerator\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.client import device_lib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2530937456958460448\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6696213545\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9961959412963312730\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Checking if TF is running on my GPU or CPU\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code solution from https://github.com/keras-team/keras/issues/4161 -- it will allow my GPU to dynamically\n",
    "# grow its memory and not crash when fitting my models.\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code inspired by muliple different CV2 uses on Stackoverflow and Kaggle\n",
    "image_size = tuple((128, 128)) #Specify what size you want your images to be here\n",
    "# The following function will convert all my images to arrays\n",
    "def arrayify(image_directory):\n",
    "    try:\n",
    "        image = cv2.imread(image_directory) # This will read in an image from the image directory\n",
    "        if image is not None: #If the image exists, do the following:\n",
    "            image = cv2.resize(image, image_size) # Resize the image to whatever image_size is defined as\n",
    "            return img_to_array(image)\n",
    "        else:\n",
    "            return np.array([]) #Else... return an empty array (doing this mostly to prevent NaNs\n",
    "                                # and other forms of data implosion)\n",
    "    except Exception: #If there is an error...\n",
    "        print (f\"Nope!! {Exception}\") #Let me know\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code inspired by multiple different instances and examples of image preprocessing on\n",
    "# Stackoverflow, Kaggle, and from General Assembly lesson examples\n",
    "def process_images(image_directory): \n",
    "    #Let's instantiate some lists!\n",
    "    image_list = []\n",
    "    label_list = []\n",
    "    try:\n",
    "        print(\"Loading Images ༼ つ ◕_◕ ༽つ\")\n",
    "        actual_folder = listdir(image_directory) # listdir will give me the directory for every folder\n",
    "            # in the given directory, I can then use these directories in my function\n",
    "        for folder in actual_folder:  \n",
    "            if folder == \".DS_Store\": #I don't want .DS_Store to be called for this function\n",
    "                actual_folder.remove(folder)\n",
    "        for image_folder in actual_folder: # For each specific image folder in the main folder...\n",
    "            print(f\"Processing {image_folder}...\")\n",
    "            formatted_image_list = listdir(f\"{image_directory}/{image_folder}/\") #Pull the images out and put them in a list\n",
    "            for image in formatted_image_list: #For each image in the image list\n",
    "                if image == \".DS_Store\":\n",
    "                    formatted_image_list.remove(image)\n",
    "            for image in formatted_image_list:\n",
    "                specific_image = f\"{image_directory}/{image_folder}/{image}\" #Create a specific image\n",
    "                # Variable\n",
    "                if specific_image.lower().endswith(\".jpg\") == True: #So long as the directory ends with\n",
    "                    # .jpg, do the following\n",
    "                    resized_array = arrayify(specific_image) # call the arrayify function on the image\n",
    "                    image_list.append(resized_array) # append the resized array to the image_list var\n",
    "                    label_list.append(image_folder) # Add the folder that the image came from as its \"label\"\n",
    "        print(\"Processing Complete\")\n",
    "        return image_list, label_list\n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "                    \n",
    "    except: #If there is an error...\n",
    "        print (f\"Nope!! {Exception}\") #Let me know\n",
    "        return None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Image Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Images ༼ つ ◕_◕ ༽つ\n",
      "Processing Pepper__bell___Bacterial_spot...\n",
      "Processing Pepper__bell___healthy...\n",
      "Processing Potato___Early_blight...\n",
      "Processing Potato___healthy...\n",
      "Processing Potato___Late_blight...\n",
      "Processing Tomato_Bacterial_spot...\n",
      "Processing Tomato_Early_blight...\n",
      "Processing Tomato_healthy...\n",
      "Processing Tomato_Late_blight...\n",
      "Processing Tomato_Leaf_Mold...\n",
      "Processing Tomato_Septoria_leaf_spot...\n",
      "Processing Tomato_Spider_mites_Two_spotted_spider_mite...\n",
      "Processing Tomato__Target_Spot...\n",
      "Processing Tomato__Tomato_mosaic_virus...\n",
      "Processing Tomato__Tomato_YellowLeaf__Curl_Virus...\n",
      "Processing Complete\n"
     ]
    }
   ],
   "source": [
    "plant_village_arrays, plant_village_labels = process_images(\"./Images/PlantVillage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20636 Images from the Plantvillage Dataset, Size (128, 128)\n",
      "Labels for the Plantvillage Dataset are as follows: \n",
      " {'Tomato_Leaf_Mold', 'Potato___Late_blight', 'Tomato_Late_blight', 'Pepper__bell___Bacterial_spot', 'Tomato__Target_Spot', 'Tomato_Spider_mites_Two_spotted_spider_mite', 'Tomato__Tomato_mosaic_virus', 'Potato___Early_blight', 'Pepper__bell___healthy', 'Tomato_Early_blight', 'Tomato_healthy', 'Tomato_Septoria_leaf_spot', 'Potato___healthy', 'Tomato_Bacterial_spot', 'Tomato__Tomato_YellowLeaf__Curl_Virus'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processed {len(plant_village_arrays)} Images from the Plantvillage Dataset, Size {image_size}\")\n",
    "print(f\"Labels for the Plantvillage Dataset are as follows: \\n {set(plant_village_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Images ༼ つ ◕_◕ ༽つ\n",
      "Processing rice_Brown_Spot...\n",
      "Processing rice_Healthy...\n",
      "Processing rice_Hispa...\n",
      "Processing rice_Leaf_Blast...\n",
      "Processing Complete\n"
     ]
    }
   ],
   "source": [
    "rice_arrays, rice_labels = process_images('./Images/Rice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3355 Images from the Rice Dataset, Size (128, 128)\n",
      "Labels for the Rice Dataset are as follows: \n",
      " {'rice_Hispa', 'rice_Brown_Spot', 'rice_Leaf_Blast', 'rice_Healthy'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processed {len(rice_arrays)} Images from the Rice Dataset, Size {image_size}\")\n",
    "print(f\"Labels for the Rice Dataset are as follows: \\n {set(rice_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combining my processed arrays and labels\n",
    "\n",
    "image_arrays = plant_village_arrays + rice_arrays\n",
    "image_labels = plant_village_labels + rice_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming my Labels into numbers using LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "lb_image_labels = lb.fit_transform(image_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageDataGenerator CNN with combined image datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating my image-data-generator\n",
    "# I used this post as a resource for my datagen https://fairyonice.github.io/Learn-about-ImageDataGenerator.html\n",
    "imagedatagen = ImageDataGenerator(\n",
    "    rotation_range=25, # Rotate each image slightly\n",
    "    width_shift_range=0.1, #Slightly shift my images so that the model can learn from them\n",
    "    height_shift_range=0.1, \n",
    "    shear_range=0.2, #I don't completely understand the concept of shearing it seems to stretch the image in certain way,\n",
    "                     # either way, it seems helpful to have.\n",
    "    zoom_range=0.2, # Add some zooming in on image\n",
    "    horizontal_flip=True) # And flip them horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining my X and y vars\n",
    "X = image_arrays\n",
    "y = lb_image_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properly converting X to arrays so they can be normalized later on.\n",
    "X = np.array(image_arrays, dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = imagedatagen.flow(X, y, save_format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing X_train and X_test\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0824 20:39:21.325320 14780 deprecation_wrapper.py:119] From C:\\Users\\JoeRo\\Anaconda3\\envs\\keras_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0824 20:39:21.378179 14780 deprecation_wrapper.py:119] From C:\\Users\\JoeRo\\Anaconda3\\envs\\keras_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0824 20:39:21.480926 14780 deprecation_wrapper.py:119] From C:\\Users\\JoeRo\\Anaconda3\\envs\\keras_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0824 20:39:21.565677 14780 deprecation_wrapper.py:119] From C:\\Users\\JoeRo\\Anaconda3\\envs\\keras_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0824 20:39:21.570684 14780 deprecation_wrapper.py:119] From C:\\Users\\JoeRo\\Anaconda3\\envs\\keras_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0824 20:39:21.591629 14780 deprecation.py:506] From C:\\Users\\JoeRo\\Anaconda3\\envs\\keras_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0824 20:39:21.802074 14780 deprecation_wrapper.py:119] From C:\\Users\\JoeRo\\Anaconda3\\envs\\keras_gpu\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0824 20:39:21.837971 14780 deprecation_wrapper.py:119] From C:\\Users\\JoeRo\\Anaconda3\\envs\\keras_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "width = 128\n",
    "height = 128\n",
    "depth = 3\n",
    "\n",
    "# Model Instantiation\n",
    "model = Sequential()\n",
    "\n",
    "# Input\n",
    "model.add(Conv2D(32, (3,3), input_shape = (width, height, depth)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Second Layer\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Third Layer\n",
    "model.add(Conv2D(128, (3,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Flatten It\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fourth Layer \n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "#Fifth Layer\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "# Final Output Layer\n",
    "model.add(Dense(19))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer='adam',\n",
    "                       metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 126, 126, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 126, 126, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 42, 42, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 42, 42, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 40, 40, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 38, 38, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 38, 38, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 19, 19, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 19, 19, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 46208)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               5914752   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 19)                1235      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 19)                0         \n",
      "=================================================================\n",
      "Total params: 6,017,491\n",
      "Trainable params: 6,017,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-808791091314>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m history = model.fit_generator(\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mimagedatagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_gpu\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py\u001b[0m in \u001b[0;36mflow\u001b[1;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, subset)\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[0msave_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_prefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m             \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m         )\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_gpu\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, dtype)\u001b[0m\n\u001b[0;32m    110\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_misc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_misc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_gpu\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    imagedatagen.flow(X_train, y_train, batch_size=32),\n",
    "    validation_data=(X_test, y_test),\n",
    "    steps_per_epoch=len(X_train) // 32,\n",
    "    epochs=25, \n",
    "    verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## While I wish I could get this imagedatagenerator to work, it seems as though my rig does not have the RAM to pull it off sadly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Keras-GPU",
   "language": "python",
   "name": "keras_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
